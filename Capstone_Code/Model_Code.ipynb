{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b1e4b9",
   "metadata": {},
   "source": [
    "### Project - Propensify\n",
    "#### Submitted by - Gayathri Keerthivasagam\n",
    "### Description:\n",
    "The end-to-end implementation of the propensity model to identify potential customers is detailed below.  \n",
    "Comprehensive exploratory data analysis (EDA) and determination of the best resampling technique, along with the optimal machine learning model for this marketing campaign, are documented in another notebook titled 'Machine_Learning_Model_Implementation.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5b612",
   "metadata": {},
   "source": [
    "### Process overview\n",
    "\n",
    "1.Data Collection  \n",
    "2.Data Cleaning and Preprocessing  \n",
    "3.Feature Engineering and Selection  \n",
    "4.Dealing with Imbalanced Data  \n",
    "5.Model Selection  \n",
    "6.Model Training and Evaluation   \n",
    "7.Save the model as pickle file  \n",
    "8.Load the model and predict for the test dataset  \n",
    "9.Append the predicted target  \n",
    "10.Save the final test file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45769b28",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf90b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "imbalanced-learn\n",
    "matplotlib\n",
    "seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67db1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#libraries for preprocessing steps\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#libraries for resampling the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#libraries for machine learning models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#libraries for cross validation,train test split,hyperparameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#libraries for performance metrics\n",
    "from sklearn.metrics import accuracy_score,classification_report,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,roc_curve,roc_auc_score,RocCurveDisplay, log_loss\n",
    "\n",
    "#libraries to regulate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa282b6d",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2093dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train.xlsx')\n",
    "test_df = pd.read_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2854b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custAge</th>\n",
       "      <th>profession</th>\n",
       "      <th>marital</th>\n",
       "      <th>schooling</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>pmonths</th>\n",
       "      <th>pastEmail</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>sep</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.199</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>0.886</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.0</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>sep</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>success</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>92.379</td>\n",
       "      <td>-29.8</td>\n",
       "      <td>0.788</td>\n",
       "      <td>5017.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.327</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.964</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.0</td>\n",
       "      <td>services</td>\n",
       "      <td>divorced</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.153</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   custAge   profession   marital            schooling  default housing loan  \\\n",
       "0      NaN       admin.   married                  NaN       no      no  yes   \n",
       "1     35.0     services   married          high.school       no      no   no   \n",
       "2     50.0  blue-collar   married  professional.course  unknown     yes   no   \n",
       "3     30.0       admin.    single    university.degree       no      no   no   \n",
       "4     39.0     services  divorced          high.school       no     yes   no   \n",
       "\n",
       "    contact month day_of_week  ...  previous     poutcome  emp.var.rate  \\\n",
       "0  cellular   sep         wed  ...         1      failure          -1.1   \n",
       "1  cellular   sep         tue  ...         1      success          -3.4   \n",
       "2  cellular   may         thu  ...         1      failure          -1.8   \n",
       "3  cellular   aug         wed  ...         0  nonexistent           1.4   \n",
       "4  cellular   nov         tue  ...         0  nonexistent          -0.1   \n",
       "\n",
       "  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  pmonths  pastEmail  \\\n",
       "0         94.199          -37.5      0.886       4963.6    999.0          2   \n",
       "1         92.379          -29.8      0.788       5017.5      0.1          2   \n",
       "2         92.893          -46.2      1.327       5099.1    999.0          2   \n",
       "3         93.444          -36.1      4.964       5228.1    999.0          0   \n",
       "4         93.200          -42.0      4.153       5195.8    999.0          0   \n",
       "\n",
       "   id  \n",
       "0   1  \n",
       "1   2  \n",
       "2   3  \n",
       "3   4  \n",
       "4   5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c7211",
   "metadata": {},
   "source": [
    "### Drop unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbaa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring the additional columns available other than the columns mentioned in the project decription.\n",
    "#Dropping the 'profit' and 'id' features.\n",
    "def train_exclude_features(train_df):\n",
    "    train_df.drop(['profit','id'],axis=1,inplace=True)\n",
    "#Delete the last two rows as it has only null values\n",
    "    train_df= train_df.iloc[:-2]\n",
    "    return train_df\n",
    "def test_exclude_features(test_df):\n",
    "    test_df.drop(['id'],axis=1,inplace=True)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17caf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_df.copy()\n",
    "new_test_df =test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f46c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_exclude_features(new_train_df)\n",
    "new_test_df = test_exclude_features(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c35d8",
   "metadata": {},
   "source": [
    "Thus the additional features such as 'profit' and 'id' are dropped as mentioned in the project description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc0f8a",
   "metadata": {},
   "source": [
    "### Null value Treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aebc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_treatment(dataset):\n",
    "# Calculate mean age for each profession\n",
    "    mean_age_by_profession = dataset.groupby('profession')['custAge'].mean()\n",
    "# Impute null values in 'custAge' based on mean age for each profession\n",
    "    for profession,mean_age in mean_age_by_profession.items():\n",
    "        dataset.loc[(dataset['profession'] == profession) & (dataset['custAge'].isnull()), 'custAge'] = mean_age\n",
    "#Mode imputation for categorical column 'day_of_week'\n",
    "    missing_cat_column = ['day_of_week']\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    dataset[missing_cat_column] = mode_imputer.fit_transform(dataset[missing_cat_column])\n",
    "#Impute null values in 'schooling' column with mode value based on 'profession'.\n",
    "    schooling_by_profession = dataset.groupby('profession')['schooling'].agg(lambda x: x.mode())\n",
    "    for profession, mode_value in schooling_by_profession.items():\n",
    "        dataset.loc[(dataset['profession'] == profession) & (dataset['schooling'].isnull()), 'schooling'] = mode_value\n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac4a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = null_value_treatment(new_train_df)\n",
    "new_test_df = null_value_treatment(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d7ca3",
   "metadata": {},
   "source": [
    "Thus the null values in the 'custAge','day_of_week','schooling' features are imputed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077de547",
   "metadata": {},
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e13ba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dataset):\n",
    "    #checking for duplicates\n",
    "    num_duplicates = dataset.duplicated().sum()\n",
    "    print(num_duplicates)\n",
    "    #Removing the duplicate records\n",
    "    dataset.drop_duplicates(inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bd0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "new_train_df = remove_duplicates(new_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fc632",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5225d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target variable for class 0 and class 1\n",
    "new_train_df['responded'] = new_train_df['responded'].map(lambda x: 0 if x == 'no' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9a7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataset):\n",
    "    dataset.drop('pmonths',axis=1,inplace=True)\n",
    "#Define conditions and choices for pdays\n",
    "    conditions = [\n",
    "        (dataset['pdays'] == 999),\n",
    "        (dataset['pdays'] < 5),\n",
    "        ((dataset['pdays'] >= 5) & (dataset['pdays'] <= 10)),\n",
    "        ((dataset['pdays'] > 10) & (dataset['pdays'] != 999)) ]\n",
    "    choices = ['not_contacted', 'less_than_5_days', '5_to_10 days', 'greater_than_10_days']\n",
    "    # Create the 'pdays' column based on conditions\n",
    "    dataset['pdays'] = np.select(conditions, choices, default='unknown')\n",
    "    \n",
    "    \n",
    "# Define conditions and choices for pastEmail\n",
    "    conditions_pastEmail = [\n",
    "        (dataset['pastEmail'] == 0),\n",
    "        (dataset['pastEmail'] < 10),\n",
    "        (dataset['pastEmail'] >= 10) ]\n",
    "    choices_pastEmail = ['no_email_sent', 'less_than_10', 'more_than_10']\n",
    "    # Create the 'pastEmail_category' column based on conditions\n",
    "    dataset['pastEmail'] = np.select(conditions_pastEmail, choices_pastEmail, default='unknown')\n",
    "    \n",
    "\n",
    "# Define conditions and choices for 'custAge'\n",
    "    conditions_custAge = [\n",
    "         (dataset['custAge'] <= 30),\n",
    "         ((dataset['custAge'] > 30) & (dataset['custAge'] <= 45)),\n",
    "         ((dataset['custAge'] > 45) & (dataset['custAge'] <= 60)),\n",
    "         ((dataset['custAge'] > 60) & (dataset['custAge'] <= 75)),\n",
    "         (dataset['custAge'] > 75) ]\n",
    "    choices_custAge = ['below_30', '30-45', '45-60','60-75','above_75']\n",
    "    # Create the 'custAge' column based on conditions\n",
    "    dataset['custAge'] = np.select(conditions_custAge, choices_custAge, default='unknown')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17bd7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = feature_engineering(new_train_df)\n",
    "new_test_df = feature_engineering(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d6518",
   "metadata": {},
   "source": [
    "Feature engineering is done for 'custAge','pdays','pmonths' and 'pastEmail' features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4fb6f",
   "metadata": {},
   "source": [
    "### Identifying numerical,categorical and binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b510185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the numerical features in the dataset\n",
    "numerical_columns = new_train_df._get_numeric_data().columns\n",
    "#Find the categorical features in the dataset\n",
    "categorical_columns = new_train_df.drop(numerical_columns,axis=1).columns\n",
    "#identify the binary columns\n",
    "binary_cols = []\n",
    "for i in new_train_df.select_dtypes(include=['int', 'float']).columns:\n",
    "    unique_values = new_train_df[i].unique()\n",
    "    if np.in1d(unique_values, [0, 1]).all():\n",
    "        binary_cols.append(i)\n",
    "numerical_columns = [i for i in numerical_columns if i not in binary_cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d9279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the skewness of the numerical features\n",
    "\n",
    "def skewness(dataset, numerical_columns):\n",
    "    # Calculate skewness of each column\n",
    "    skewness = dataset[numerical_columns].skew()\n",
    "    \n",
    "    # Find columns with positive skewness\n",
    "    positive_skew_cols = skewness[skewness > 1].index.tolist()\n",
    "    print(positive_skew_cols)\n",
    "    \n",
    "    # Apply log transformation to columns with positive skewness\n",
    "    for col in positive_skew_cols:\n",
    "        dataset[col] = np.log1p(dataset[col])\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079b97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['campaign', 'previous']\n",
      "['campaign', 'previous']\n"
     ]
    }
   ],
   "source": [
    "new_train_df = skewness(new_train_df,numerical_columns)\n",
    "new_test_df = skewness(new_test_df,numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4f684",
   "metadata": {},
   "source": [
    "### Feature Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf0b2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_encoding(dataset):\n",
    "# Initialize LabelEncoder    \n",
    "    label_encoder = LabelEncoder()\n",
    "#specify the features that needs to be label encoded\n",
    "    cat_cols1 = ['profession','schooling', 'month', 'day_of_week']\n",
    "    for col in cat_cols1:\n",
    "        dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "#specify the features that needs to be one hot encoded        \n",
    "    cat_cols2 = ['custAge','marital', 'default', 'housing', 'loan','contact', 'poutcome','pdays','pastEmail']\n",
    "# Use pd.get_dummies() to one-hot encode the categorical columns\n",
    "    encoded_features = pd.get_dummies(dataset[cat_cols2])\n",
    "# Concatenate the original DataFrame with the encoded features along the columns axis\n",
    "    dataset = pd.concat([dataset, encoded_features], axis=1)\n",
    "# Drop the original categorical columns \n",
    "    dataset.drop(cat_cols2, axis=1, inplace=True) \n",
    "\n",
    "    return dataset\n",
    "\n",
    "new_train_df = feature_encoding(new_train_df)\n",
    "new_test_df = feature_encoding(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4483b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for scaling the numerical features\n",
    "def feature_scaling(dataset, numerical_columns):\n",
    "    sc_x = StandardScaler()\n",
    "    dataset[numerical_columns] = sc_x.fit_transform(dataset[numerical_columns])\n",
    "    return dataset\n",
    "\n",
    "new_train_df = feature_scaling(new_train_df, numerical_columns)\n",
    "new_test_df = feature_scaling(new_test_df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e17e8",
   "metadata": {},
   "source": [
    "### Split the dataset into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a5d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the features and target\n",
    "X = new_train_df.drop('responded',axis= 1)\n",
    "y = new_train_df['responded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10bf5c",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9358a2",
   "metadata": {},
   "source": [
    "After conducting a thorough analysis in the Python notebook titled 'Machine_learning_model_implementation.ipynb', we determined that the optimal machine learning model for this dataset is 'GradientBoosting + SMOTE'. We are now deploying this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab27887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation with SMOTE technique\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=24)\n",
    "smote = SMOTE(random_state=12)\n",
    "gb_classifier = GradientBoostingClassifier(random_state=12)\n",
    "pipeline = Pipeline([('smote', smote), ('gb_classifier', gb_classifier)])\n",
    "\n",
    "# Define the parameter grid to search\n",
    "parameters = {\n",
    "    'gb_classifier__n_estimators': [100, 200, 300],\n",
    "    'gb_classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb_classifier__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           param_grid = parameters, \n",
    "                           cv=skf, \n",
    "                           scoring='f1', \n",
    "                           n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_est = grid_search.best_estimator_\n",
    "best_model = grid_search.best_estimator_['gb_classifier']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac6f38",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d728a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6409759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069906a0",
   "metadata": {},
   "source": [
    "### Predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c736f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = loaded_model.predict(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8b8e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33074ae",
   "metadata": {},
   "source": [
    "### Predictions are added to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b0f1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the test dataset\n",
    "test_predictions['responded'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce6cfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "responded\n",
       "0    23347\n",
       "1     9603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions['responded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4855e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target 'yes' for 1 and 'no' for 0\n",
    "test_predictions['responded'] = test_predictions['responded'].replace({0: 'no', 1: 'yes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8800c7",
   "metadata": {},
   "source": [
    "### Save the final test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa8668a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the updated test dataset\n",
    "test_predictions.to_excel('test_predictions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201f996",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    " Utilizing gradient boosting algorithm along with the SMOTE technique, potential customers for the insurance company have been successfully identified with an accuracy of 0.87. This achievement enables the company to target their potential customers effectively, leading to success in their marketing campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0177f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
